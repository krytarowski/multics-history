.ifi init_plm "SIB12.3"
.srv section 5
.srv draft_date ""
.srv draft "Instructions - First Time"
.ifi l0h "Instructions for Sites Installing for First Time"
The following basic procedure must be performed when installing Multics
for the first time.  
.ifi l1h "Step 1: Preparation"
.spb
Ensure that all Multics active hardware components run error free in Multics
mode using the latest T&D release.  Peripheral equipment can be run in either
Multics or GCOS mode and must also run error free.
.spb
Carefully  check  the hardware configuration (port and channel assignments,
mailbox   switch  settings,  etc.)  Create  and  verify  the  configuration
description   on  paper  for  later  input  when  BCE  is  running.   Close
consultation between the SiteSA and Field Engineering representative is of
the  utmost  importance.   (Refer to Section 9 and Appendix A of the
Operator's Guide to Multics, Order Number GB61, for hardware switch setting
information.  Refer to Section 7 of the Multics System Maintenance Procedures,
Order Number AM81-03, for configuration setup.)
.spb
When selecting the storage unit for the RPV, select a disk unit with as few
bad tracks as possible.  For the MSS451s, T&Ds should be used to format and
test the first disk to be used as the RPV (test 365, subtest 26).

The MSS500/501s are formatted at the factory, however, selection of alternate
tracks is not done at the factory.  It can only be done using MTR at Multics
command level.
(Refer to Appendix B for an outline of how MTR runs under Multics.)
.brp
.ifi l1h "Step 2: Logical Volume Assignments"
.spb
Choose the logical volume assignments.  Decide how many logical volumes are
needed and how many physical volumes are to be in each.
.spb
Most installations have the following:
.inl +5
.spb
Logical Volume	Contents
.spb
.inl +20
.unl 20
root                >system_control_1
.brf
>system_library_standard
.brf
>system_library_tools
.brf
>system_library_unbundled
.brf
>system_library_auth_maint
.brf
>system_library_1
.brf
>documentation
.brf
>daemon_dir_dir
.brf
>dumps
.brf
>system_library_tandd
.brf
>system_library_obsolete
.brf
>system_library_3rd_party
.brf
>site
.brf
>lv
.brf
partitions
.spb
.unl 20
public		>user_dir_dir
.brf
>library_dir_dir>include
.spb
.unl 20
ldd                 >library_dir_dir 
.inl -20
.spb
.inl -5
Other logical volumes may be set up for specific applications.
.spb
.brn 11
The assignment decision requires the system administrator to balance the costs
of seek interference and breakage against the advantages of being able to
define and process logically different collections of data.  Data bases used
for only a few hours a day or only a few days a month are natural candidates
for allocation to a separate logical volume.  Breaking up the system's storage
into several logical volumes also allows the site to operate without all
logical volumes mounted if hardware goes down.  For example, an MPC or channel
might go down, halving the system's disk drive capacity.
.brp
Logical volume assignments might be as follows:
.spb 2
.inl +5
Logical Volume      Contents
.spb
.fif
.inl +20
.unl 20
root                >system_library_tandd
>library_dir_dir
>system_library_obsolete
>system_library_standard
>system_library_tools
>system_library_unbundled
>system_library_3rd_party
>daemon_dir_dir
>documentation
>dumps
>system_library_1
>system_library_auth_maint
>user_dir_dir>Daemon
>user_dir_dir>SysAdmin
>user_dir_dir>SysDaemon
>user_dir_dir>SysMaint
>site
>lv
partitions
.spb
.unl 20
Mcc                 >user_dir_dir>Mcc
.spb
.unl 20
Multics_Pubs        >user_dir_dir>Pubs
>user_dir_dir>Multics
.spb
.unl 20
Old_Dumps           >dumps>Old_dumps
.spb
.unl 20
Public              >user_dir_dir
>experimental
>process_dir_dir
>ldd>include
.spb
.unl 20
list_1              >library_dir_dir>listings>hard
>library_dir_dir>mcs
>library_dir_dir>unbundled
.spb
.unl 20
list_2              >library_dir_dir>listings
.spb 1

.inl 0
.fin
These particular assignments give a wide range of flexibility and Multics can
run with only the root logical volume mounted, or with one or two of the less
critical logical volumes not mounted due to unavailability of disk drives.
For example, logical volumes, list_1 and list_2, can easily be demounted.
This frees two disk drives to be available for use with other more critical
logical volumes.
.spb
Installations that wish to use the Access Isolation Mechanism (AIM) by
specifying more than one access category (sensitivity level) should specify
the maximum and minimum categories for one or more volumes and thus ensure
that sensitive data is confined to a few packs, or that packs are not
"contaminated" with information requiring special precautions.
.spb
The logical volumes that hold process directory segments must be chosen.
Because of the heavy usage of process directory segments, these segments
should be spread over as many physical volumes as possible.  One or more
logical volumes may be selected to hold process directory segments, using the
set_pdir_volumes command in system_start_up.ec.  In the supplied
system_start_up.ec, a single logical volume, named public, is selected.  This
command line should be changed to select a set of publicly accessible and
permanently mounted logical volumes containing as many physical volumes as
possible, subject to some constraints.  Site maintenance personnel are
responsible for ensuring there is always enough space available on the
selected logical volumes to hold the process directory segments.  The process
directory placement algorithm causes process directory creations to be made on
each logical volume in proportion to the number of physical volumes in the
logical volume.
.spb
Ensure that enough storage will be available.  About 5% of each volume is used
for the VTOC and volume map.  In addition, some breakage is unavoidable.
.spb
Since the system handles running out of storage without crashing, and since it
is possible to add physical volumes to a logical volume dynamically, logical
volumes can be defined with fewer physical volumes than their maximum
anticipated size.
.ifi l1h "Step 3: RPV Initialization"
Mount the Multics System Tape (MST) on Magnetic Tape Handler (MTH) nn (nn is
usually equal to 01).  Mount the disk pack formatted by T&D on the drive
selected to be the RPV.  Initialize and boot the MST.  Multics will prompt
with:
.spf
.fif
bootload_0:  Booting system MR12.3 generated 08/31/89 0000.0 
bootload_0:  Enter boot tape MPC model:  ! t500
.fin
.spf
Normal response to this question should be "ipc", "t610", "t601" or "t500".
However, on systems with an IMU configured the "Enter boot tape..." query will
not appear.  The system will boot the bootload tape controller, if necessary,
and continue.  At this time, the intention to cold boot is given.  Multics
will request the location of the rpv.  Once this is done, the init_vol request
loop will be entered to accept the layout of the rpv.
.spf
.fif
bootload_0:  Booting t500 A 12.3.  with mtc500 rev.u1 firmware.
bootload_0:  Booted tape MPC.
0000.1 announce_chwm: 371. pages used of 512. in
		         wired environment.
0000.2 announce_chwm: 646. words used of 1024. 
                             in int_unpaged_page_tables.
find_rpv_subsystem: Enter RPV data: M-> ! query
find_rpv_subsystem: Enter RPV subsystem base channel, as Icc, 
		         or "cold". M-> ! cold
find_rpv_subsystem: Booting cold will destroy all data on the RPV
   Are you sure that you want to boot cold?  M-> ! yes
find_rpv_subsystem: Enter RPV subsystem base channel,
                             as Icc. M-> ! a22
find_rpv_subsystem: Enter RPV subsystem MPC model: M-> ! 451
find_rpv_subsystem: Enter RPV disk drive model: M-> ! 451
find_rpv_subsystem: Enter RPV drive device number: M-> ! 1
find_rpv_subsystem: RPV is a model 451 drive, number 1 on MPC A22
	                   (Model 3), and this is a COLD boot.
    Is this correct? M-> ! yes
.spf
Default RPV layout:  (Respond "end" to use it.)
Average seg length = 2.00
VTOC size = 2792 pages, 13920 vtoces.
27840 paging records.
Constrained by average seg length.
part hc 2792.  2500.
part conf 5292.  4.
part alt 38117.  141.	(451 disk drive only)
part dump 35847.  2000.
part log 35591.  256.
part file 35336.  255.
part bce 33136.  2200.
request: M-> ! end
.spf
.fin
These are the default partition  assignments.  Any changes to the
default partitions  or RPV parameters  can be redefined  by using
the "startover" request in init_vol.  The system installer should
review  the  write-up  of  init_vol  in  the  Multics Administration,
Maintenance,
and Operations Commands Manual, Order Number GB64-00,  prior  to  the
installation.

Sizes  for  the various  partitions  and their  locations  can be
modified based on the needs of the site.
.fif
.spf
init_empty_root:  Begin rpv initialization.  This will 
     take some time.
init_empty_root:  rpv initialized; 27840 records.
bce (early) 0012.0: M->
.fin
.spf
The list above is based on a 451 disk drive.  With the exception of some
default values the script will be the same for 501, 3380, or 3381 disk units.
.inl 0
.ifi l1h "Step 4: Configuration"
Build the configuration description as follows (user input preceeded by an
exclamation mark (!):
.spf
.inl 5
.fif
! config
! 1,$d
! a
! [User types in configuration fields as defined
   in the System Maintenance Procedures, Order 
   Number AM81-03]
! \f
! w
! q
.spf
.inl 0
.fin
Do  not  enter any  part  cards at  this  time, except  for those
partitions defined on the rpv.   Also, make the root card specify
only the rpv.
.spf
Continue booting bce.
.spf
.fif
.inl 5
bce (early) 0020.0: M-> ! bce
Current system time is:  Monday, April 29, 1985 00:20:46 mst
Is this correct?  no
Enter time: M-> ! 12/01/89__13:21:30
Current system time is:  Friday, December 01, 1989 13:21:30 mst
Is this correct? M-> ! yes
load_disk_mpcs:  Disk mpc(s): mspa mspc appear not to be
      operating.
Enter disk mpc names to be loaded, or "none" or "abort" 
      or "all": M-> ! mspa mspc
.spf
(The operator entered the names of other disk mpcs
 to be loaded.)
.spf
hc_load_mpc:  Booting channel A20 with dsc500 Revision X1.
hc_load_mpc:  Booting channel B20 with dsc500 Revision X1.
bce (boot) 1325.5: M->
.fin
.inl 0
.spf
At  this time,  the operator  must load  firmware into  all other
controllers (i.e., not the bootload  tape controller nor any disk
controllers).  bce is then considered to be fully initialized.
.fif
.inl 5
.spf
bce (boot) 1325.5 : M-> ! boot -cold
Do you really wish to boot cold and there by destroy the
     system hierarchy? M-> ! yes
1326.1 volume_registration_mgr_$check_volume_registration: 
        Reregistered public LV root LVID 133353533031 
        (Initializer.SysDaemon.z)
1326.3 volume_registration_mgr_$check_volume_registration: 
        Reregistered PV rpv PVID 133353533017 in LV root 
        (Initializer.SysDaemon.z)
disk_table_: New disk_table created
Multics MR12.3 - 12/01/89 1327.0 mst Fri.
Command: M->
.spf
.inl 0
.fin
Ignore the messages prefaced by disk_table_ and volume_registration_mgr_.
.ifi l1h "Step 5: Initializing Root Volumes"
Initialize each new root volume except the RPV with the init_vol command.
.spf
For better performance, it is advisable to place a hardcore partition
(hc) on each physical volume of the Root Logical Volume (RLV).  The placement
of the hardcore partition on each volume must be low.  The recommended size of
additional partitions is
2500 records divided by the number of physical volume used. The RPV size should
remain 2500 records to allow the system to boot with only an RPV mounted.

For most volumes the command looks like:
.spb
.inl +5
.fif
init_vol PV_NAME DRIVE_NAME  -rlv {-special}
.spb
Example: init_vol root2 dska_02 -rlv -special
.inl 0
.fin
.brn 15
.spb
For those volumes that are to have partitions, or an average segment length
other than the default of five records, add the optional "-special" as a third
argument.  The command then asks for instructions about the partition
location.  Hardcore partitions, for additional root volumes, should be
specified as they are initialized.  You may type one or more of the following:
.spb
.inl +5
part NAME low nrec
.brf
part NAME high nrec
.brf
avg fff.ff
.brf
list
.spb
.inl 0
complete initialization by typing:
.spb
.inl +5
.brf
end
.inl -5
.spb
An example of typing the init_vol for an MSS0451/400 with an alternate
partition on a RLV drive is:
.spb
.inl 5
.fif
init_vol root2 dska_02 -rlv -special
part alt high 141	 (Note: 451 disk only)
part hc low 625	 (Example: 2500/4 root volumes)
end
.spb
.inl 0
.fin
when done type:
.spf
     shut
.ifi l1h "Step 6: Additional Configuration Parameters"
At bce (boot) level enter the configuration deck editor by typing "config".
The PART cards and ROOT card should be added to the deck.  Subsequent boots
divide the hardcore supervisor among all hardcore partitions.
.spb
The following script is provided as an example where a root card exists in
the configuration deck and a part card does not exist.  Parameters of
cards will vary according to the configuration of individual sites.  User
input is preceeded by an exclamation mark (!).
.spf
.fif
.inl 5
bce (boot): M-> ! config
M-> ! /root/
root -subsys dski -drive 1
M-> ! s/$/ -subsys dski -drive 2/p
root -subsys dski -drive 1 -subsys dski -drive 2
M-> ! /part/
Search failed.
M-> ! a
M-> ! part bos dski 1
M-> ! part log dski 1
M-> ! part dump dski 1
M-> ! \f
M-> ! w
M-> ! q
bce (boot) 1215.2: M-> reinit
.inl 0
.fin
.ifi l1h "Step 7: Reload of Executable Libraries"
Do a normal boot "BOOT".  While at ring-1 initializer command level load the
executable libraries.  This is done as follows:
.spb
.fif
.inl 5
bce (boot): ! boot
Command: M-> ! reload -nosetlvid
.fin
.spb
.inl 0
Only the system libraries (MR12.3.EXEC) should be reloaded at this time.
The -nosetlvid control argument ignores the logical volume
ID on the tape when a directory is being reloaded.
.spb
.fif
M-> ! shut
1230.1 shutdown complete.
bce (boot) 1231.1: M-> ! boot standard 
		(ignore the messages from sc_init_.)
Multics MR12.3 - 12/01/89 1235.2 mst Fri
Ready
M-> ! admin    
admin: Entry not found.  Could not retrieve admin password
  from the PNT to check admin password.  Entering admin mode.
   [NOTE: This error message is repeated each time admin is 
          entered until a password has been set.]
.fin
.inl 0
.spb
Register and initialize all non-RLV volumes.  For ease of typing, use of lower
case names is recommended.
.spf
Use the add_volume_registration (avr) command as in the following 
example:
.spb
.inl 5
.fif
! avr -pv pub01 -lv public -serial 233-81 -model 451
add_volume_registration: LV "public" does not exist.  Do you
     wish to create it? M-> ! yes
add_volume_registration: Registered PV "pub01" (pvid
     100172223140) on new LV "public" lvid 100172223005).
r 14:15  1.473 8
.inl 0
.spf
.fin
to create registration entries for each logical and physical volume.  The
registration file for the root logical volume is created automatically by the
bootload.  Since the default model number is 451, use the
change_vol_registration command, if necessary, to set the correct value of
model number on the rpv.  The serial number can also be set as follows:
.spb
.fif
lvr -pv rpv
cvr -pv rpv -serial 233-79 -model <model number>
ame
.spf
Use the init_vol for the additional logical volumes as follows:
.spf
.fif
init_vol pub01 dska_03 -special
part alt high 141	(Note: 451 disk only) 
end
.brf
init_vol pub02 dska_04 -special
part alt high 141	(Note: 451 disk only)
end
.fin
.spb
After all physical volumes are registered and initialized, add them to the
disk_table by typing the add_vol (av) command for all except the RPV:
.spf
     av pvname dskX_NN
.spf
An example:
.spf
     av pub01 dska_03
.spf
At this point add all of the logical volumes by typing:
.spf
     alv -all
.brp
.ifi l1h "Step 8: Setting and Checking Access"
The ACL for >lv should be set to "s" for all users.  Setting initial ACLs for
segments in the >lv directory is done from admin mode by typing:
.spb
.inl 5
.fif
M-> ! admin
<READY MESSAGE>
M-> ! sis >lv rew *.SysAdmin rew *.SysDaemon
<READY MESSAGE>
M-> ! sa >lv s * sma *.SysAdmin sma *.SysMaint
<READY MESSAGE>
.fin
.inl 0
.spb
Create Access Control Segments (ACS) for each logical volume.  For system
public volumes, create them as follows:
.spb
.inl 5
.fif
M-> ! create >lv>{lvname}.acs
<READY MESSAGE>
M-> ! cvr -lv {lvname} -acs >lv>{lvname}.acs
<READY MESSAGE>
M-> ! set_max_length >lv>*.acs 0
.fin
.spb
.inl 0
where {lvname} stands for name of each logical volume.
.spf
The ACLs of these segments are interpreted to give permission to attach the
logical volume (for private volumes) and permission to modify master directory
control information in the MDCS (for specific logical volumes).  Specific ACL
entries for Initializer.SysDaemon should be deleted at this time by typing:
.spb
.inl +5
delete_acl >lv>*.acs
<READY MESSAGE>
.spb
.inl -5
This is necessary because Initializer.SysDaemon always gets default access of
"rw".  This would prevent the Initializer from being a volume administrator by
virtue of the missing "e" access.  Deletion of specific access gives the
Initializer the "rew" access allowed all SysDaemons.
.spb
.brn 4
The ACL is now set so that all system administrators and all SysDaemons are
volume administrators.  The "e" bit controls executive access.
.spb
For private volumes, the ACS is in a directory controlled by the volume owner.
The ACS segment must reside in a directory on a logical volume different from
the private logical volume.
.brp
.ifi l1h "Step 9: Setting Volume Quota"
Use the set_volume_quota command to give the Initializer process enough quota
on each logical volume to create the necessary master directories.
.spb
.inl +5
set_volume_quota LV_NAME QUOTA
.inl -5

Example: set_volume_quota public 36000 
.spb
The number QUOTA should be at least the total of the quotas of the directories
to be created in the next step.
.spb
Use create_dir to create master directories.
The format of the command is:
.spb
.inl +5
create_dir pathname -lv logical_volume -quota QQ
.spb
.inl -5
where QQ <262144
.spb
.inl +5
Example:	cd >library_dir_dir -lv no_backup -quota 40000
.brf
	cd >library_dir_dir>include -lv public -quota 3000
.inl -5
.spb
The acct_start_up.ec will, in step 11, create a number of project directories
and assign terminal quota if the directories do not exist.  They are the
following with the quota that will be assigned:
.fif
.spb
.inl +5
>udd>SysAdmin        5000
>udd>SysAdmin>admin  2000
>udd>SysDaemon       5000
>udd>Daemon          1000
>udd>Operator         100
>udd>Terminals         10
>udd>HFED            5000
.inl -5
.fin
.spb
Set ACLs and additional names, as desired, on all master directories at this
time.
.spb
.fin
Directory quota should be set for each master directory by those sites that
wish to have disk charges for directory pages included in monthly bills.
Those sites not interested in implementing this feature may type "ame" and
"shutdown" then skip to the next step.
.brp
A directory quota of 1000 pages should be sufficient for all master
directories with the exception of >udd.  If udd is a master directory it is
recommended that it be given a directory quota of 100000 pages.  This provides
enough directory quota for 100 projects at 1000 pages each.  If the site has
more than 100 active projects the 100000 figure should be adjusted
accordingly.  To set directory quota on each master directory execute the
following command:
.spb
     set_dir_quota PATHNAME  QQ
.spb
This command allows a system administrator to place an arbitrary
secondary storage quota for directories on a specified directory.
.fif

PATHNAME
   is the name of a directory on which the directory quota is to
   be placed.  -wd can be used to specify the working directory.
QQ
   is the directory quota in 1024 word pages.
.fin

If additional directory quota is required for a master directory the quota
can be reset following movement of directory quota to inferior directories.
.spb 2
Instructions for moving directory quota down to the project level is included
in Step 18.
.spb
To shut down do the following:
.spb
.inl 5
ame
.brf
shut
.inl 0
.ifi l1h "Step 10: Reload of Remaining Release Tapes"
Reboot Multics to ring-1 and reload the MR12.3.LDD_STANDARD and
MR12.3.UNBUNDLED tapes with the following commands:
.spb
.inl 5
     boot 
.brf
     alv -all
.brf
     reload -nosetlvid
.spb
.inl 0
The tape labeled MR12.3.MISC 
must be the final tape of the MR12.3 supplied set to be reloaded.  
.brp
.ifi l1h "Step 11: Running acct_start_up.ec"
After all the release tapes have been reloaded cross into ring-4 by executing the following commands:
.spb
.fif
.inl 5
standard
.brf
admin   [Ignore messages from admin at this time.]
.spf
.fin
.inl 0
At this time you are ready to execute part 1 of the acct_start_up.ec.
To do this type:
.spf
ec >system_library_tools>acct_start_up cold F.ANSS
.spf
where F.ANSS is the channel number of the hardwired Initializer terminal.
.spb
.inl 5
     F = FNP number (a-h)
     A = Adaptor type (h = hsla)
     N = Adaptor number (0-2 for hsla)
    SS = Decimal subchannel number of specified adaptor
.spb
.inl 0
The string "F.ANSS" should be replaced by "otw_" if there is no hardwired
terminal and the bootload console is to be used as the Initializer terminal.
.spb
The error, "new_iod_tables_compiler: Entry not found. Accessing cdt. Channel
name checks will not be performed", may be encountered during this stage.
This error should be disregarded.
.spb
At this point, tapes dumped from other Multics sites can be reloaded as
desired using the "reload" command with the control arguments "-noquota
-notrim -nosetlvid" to avoid deletion of existing segments and resetting of
quotas. If any segments are to be loaded into ring 1 then it cannot be done without exiting admin mode and
rebooting to ring 1.
.ifi l1h "Step 12: Multics Communications System"
A Multics Communications System (CS) core image is supplied in
the >unbundled library, and is named site_mcs.
.sp
The site_mcs version for this release is 7.6.  The site_mcs core image
contains the basic support for DN6780 type FNPs with 64k of memory.
.spb
The communication system core images are built using the bind_fnp command in
conjunction with a bindfile describing the CS modules and configurations to be
used.  A copy of the site_mcs.bind_fnp can be found in >ldd>mcs>info.
.spb 1
Sites should build their own CS core image tailored to their own FNP
configuration, terminal type requirements, and use of additional separately
priced FNP software modules.
.spb
Sites using the default site_mcs core image should ensure the CMF image
statement points to the correct default core image located in >unb.  The
initial CMF, which includes some sample channel entries in comments, as well
as one FNP entry, should be checked.  Edit this CMF to eliminate any
inconsistencies with the actual configuration and add one or more entries
for login channels.
.spb
Sites modifying their own CS
core image are required to use the GCOS Environment Simulator which is an
unbundled software product.  To build a new core image, the following
procedure is suggested:
.spb
Sites will need to extract the communications object segments from
archives located in
>ldd>mcs>object. Sites should create a virgin directory
under >udd>sa>a for each new core image.

The following example is for sites with the more common type Datanet and a
larger memory configuration.  Execute the following commands:
.spb
.fif
     create_dir >udd>sa>a>mcs.7.6c
     cwd >udd>sa>a>mcs.7.6c
     ted
     r >ldd>mcs>info>site_mcs.bind_fnp (or location of sites
                                         CS bind file)
     .
     .
     .
     make editing changes if any..  
     .
     .
     .
     w site_mcs.bind_fnp
     q
     ac x ([segs >ldd>mcs>o>*.archive -absp])
     bind_fnp site_mcs -list
.spb
.fin
Be sure the image statement in the CMF points to this newly created CS core
image.  The following example assumes the default CMF to be in the >udd>sa>a
directory.  This procedure will insure that the new CS image is used:
.spb
.inl 5
.fif
cwd >udd>sa>a
ted
r  CMF.cmf
.
.
Locate the image: statement by typing:

/image/

Edit the image: statement to point to the CS image,
site_mcs, by making the statement read:

:image:  >udd>sa>a>mcs.7.6c>site_mcs;

.
.
Make any other changes needed.
.
.
w CMF.cmf
q
cv_cmf CMF.cmf
copy CMF.cdt >sc1>cdt -force
.fin
.inl 0
.spb
The above procedure builds a site dependent CS core image and ensures that
this image is loaded in the FNP by the answering service.
.spb 2
.ifi l1h "Step 13: Complete Accounting Startup"
.spb
During this step, expect many messages, some with audible alarms, reporting
that certain segments do not exist and are being created.  These messages
would be cause for concern during normal system operation but are to be
expected during accounting start up and may be ignored. Execute the following
example:
.spb
.inl 5
.fif
ame
stop_mpx a  (sites with multiple FNPs execute this
             command for each FNP)
multics
load_mpx a -check  (sites with multiple FNPs execute
                    this command for each FNP)
admin    (any error messages displayed at this time, except 
          hardware error messages can be ignored.)

ec >tools>acct_start_up cold2
.spb
.fin
.inl 0
This procedure will finish accounting start up.  The load_mpx command
indicates on the FNP console any configuration errors if console_man is
loaded and "console: yes;" is in the bind_file.  If any errors are reported
they should be corrected.
.spb
During this stage you will encountered the error, "set_max_length: Validation
level not in ring bracket. >system_control_1>mcaa.acs", which occurs if an IMU
is configured.  Ignore this error message.  The cause of this error,
validation level of >sc1>mcaa.acs, must be fixed manually after cold2 has
completed successfully.
.spb
A default start_up.ec is available for use by new Multics users when they
first log in.  This exec_com is executed by users who login to the system
without their own start_up.ec. The segment >tools>start_up.ec was copied
into >sc1 by the acct_start_up.ec.  Individual sites can modify this exec_com to meet their own needs.
The access for the segment should be "r *.*.*" and ring brackets of 4,5,5.
To start the system up for normal service type:
.spb
.inl 5
.fif
ame
word login
abs start
go
.fin
.spb
.inl 0
After typing "go" a number of messages will be returned.  These messages are
of the form:
.spf
.fif
absentee_utility_: Entry not found. Creating new <pathname>.
.brf
scavenge_vol:  No volumes.
.brf
Found unexpected command_error in system start_up.ec.
.spf
These messages may be ignored.
.spb 2
.ifi l1h "Step 14: Check Gate ACLs"
.spb
.fin
Type "admin" and enter the new admin password you selected.
Check the ACLs for >sss>dm_admin_gate_, >sss>dm_daemon_gate_,
>tools>installation_tools_, >tools>pnt_admin_gate_, >tools>pnt_login_gate_,
>tools>pnt_network_gate_, >tools>pnt_priv_gate_, >sss>metering_gate_ and
>sss>queue_admin_.  The ACLs on these gates are as they appear on System M and
should be restricted.  The ACL for these gates are site dependent and should
be changed to meet each site's needs.  The dm_admin_gate_ and dm_daemon_gate_
should be restricted to data management administrators or daemons.  The
installations_tools_ gate should be restricted to system library maintainers.
All persons on the ACL for metering_gate_ have access to the
Multics metering data.  All persons on the ACL for queue_admin_ are permitted
to move absentee and daemon requests for themselves and other users to
different queues.  Users not on this ACL are only able to move their own
requests.  The Initializer must have access to queue_admin_.  This capability
is also dependent on extended access to the <queue>.ms segments.
The various pnt_<name>_gate_ acls should be restricted to the following
minimums for correct system operation:
.fif
.inl 5
.spf
  >t>pnt_admin_gate_
re    *.SysDaemon.*
re    *.SysAdmin.*

  >t>pnt_fs_gate_
re    *.*.*

  >t>pnt_login_gate_
re    Initializer.SysDaemon.*
re    *.SysAdmin.*

  >t>pnt_network_gate_
re    Initializer.SysDaemon.*
re    IMFT.Daemon.*
re    Card_Input.Daemon.*
re    *.SysDaemon.*

  >t>pnt_priv_gate_
re    Initializer.SysDaemon.*
r     *.SysDaemon.*
re    *.SysAdmin.*
.inl 0
.fin
.spb
Set ACLs on the >sc1>rcp directory and on the access control segments in it
(<name>.acs), to allow users to attach tape drives and any other peripherals they
are allowed to use.
.spb
After all ACLs are set, type:
.inl 5
.spb
ame
.brf
x repair salvquota > 2 -dcf -rebuild
.brp
.inl 0
.ifi l1h "Step 15: Save Checkpoint"
.spb
Type "logout * *" and "shutdown".  After a successful shutdown, do a BCE save.
For further information see >doc>ss>bce>save.info or Appendix B of this
document.  Use fresh tapes for the BCE save so that the results of the above
steps are not lost.
.spb
.ifi l1h "Step 16: Tailor System and Register Projects"
.spb
Following the bce save, reboot the system for normal service by typing "boot
star".  The system is now ready for registration of projects and users from a
SysAdmin process, such as Repair.SysAdmin, logged in from a standard terminal.

The acct_start_up exec_com created default system_start_up.ec, admin.ec,
iod_tables.iodt, RTMF.rtmf, and CMF.cmf segments.  These segments should be
tailored by the local Site SA to meet site operational and configuration
requirements.
.spb 2
.ifi l1h "Step 17: Setup Volume Backup/Reloader"
.spb
The following instructions are necessary only for those sites that intend to
use the Volume Backup/Reloader facility:
.spb
The personids "Volume_Dumper", "Volume_Reloader", and "Volume_Retriever" are
registered.  These personids are registered on the Daemon project with the
multip and daemon attributes and with a home_dir of
>user_dir_dir>Daemon>Volume_Dumper.  Sites using AIM must set the
authorization for these personids at system_high and upgrade the home_dir at
system_high.
.spb
Login Repair SysDaemon, or if running in special session using the
Initializer, execute the following commands:
.spf
     admin
.spb
     ec >tools>setup_volume_reloader
.spb
The error message that Volumes dm001 and dm002 are not registered should be
ignored.  This exec_com creates all directories, segments, and message
segments necessary for running the volume dumper/reloader system.  This
exec_com also sets suggested access on the directories and segments created.
Not all the access set is required.  If a site wishes, the access created for
*.SysMaint.* and *.SysAdmin.* may be removed.
.spb
This exec_com resets the vtoce fields for both incremental and consolidated
dumps by making a first dump pass with output to discard_.  This is necessary
since the first dump pass is equivalent to a complete dump on both the
incremental and consolidated pass.  Follow the instructions for normal use of
this facility at the completion of this exec_com.
.spb
Sites need a sufficient number of tapes to accommodate the entire file system
and any incremental and consolidated dumps until a subsequent complete dump is
taken.  This is known as a reload group.  It is suggested that new sites start
with 100 reels of tape or a sufficient quantity to contain two complete reload
groups.  A single reel of tape at 6250 bpi holds approximately 26000 Multics
records.
.spb 2
.ifi l1h "Step 18: Setup Directory Quota"
.spb
.fin
This step is necessary only for those sites that wish to charge their
user projects for disk storage used by directory pages, or to obtain a
more complete disk report containing additional disk usage statistics.
If directory quota is not already set on >udd execute the following
commands from a SysAdmin process if udd is not a master directory:
.fif
.spb
     sac set_dir_quota > 120000
     sac move_dir_quota >udd 100000


If udd is a master directory, then execute:

     set_dir_quota >udd 100000

Then execute the commands:

     cwd >udd
     move_dir_quota ([dirs **]) 1000
.spb
.fin
These commands move or set sufficient directory quota on udd for 100
projects with the suggested default project directory quota of 1000.  If
a site has more than 100 active projects a figure in excess of 100000
must be chosen for the initial directory quota of udd.  The master.ec
gives each new project a default directory quota of 1000 pages by moving
1000 pages of directory quota from udd.  The system administrator should
make sure there is always sufficient directory quota on udd to
accommodate new projects.

It is also suggested that all directories directly off the root with the
exception of pdd and sl1 be given nonzero segment and directory quotas
large enough to accommodate their current page usage and allowing for
some growth.  The purpose of this is to cause the disk report to contain
complete statistical information on these directories (directories with
0 quotas are omitted from the disk report).


                                          -----------------------------------------------------------


Historical Background

This edition of the Multics software materials and documentation is provided and donated
to Massachusetts Institute of Technology by Group BULL including BULL HN Information Systems Inc. 
as a contribution to computer science knowledge.  
This donation is made also to give evidence of the common contributions of Massachusetts Institute of Technology,
Bell Laboratories, General Electric, Honeywell Information Systems Inc., Honeywell BULL Inc., Groupe BULL
and BULL HN Information Systems Inc. to the development of this operating system. 
Multics development was initiated by Massachusetts Institute of Technology Project MAC (1963-1970),
renamed the MIT Laboratory for Computer Science and Artificial Intelligence in the mid 1970s, under the leadership
of Professor Fernando Jose Corbato. Users consider that Multics provided the best software architecture 
for managing computer hardware properly and for executing programs. Many subsequent operating systems 
incorporated Multics principles.
Multics was distributed in 1975 to 2000 by Group Bull in Europe , and in the U.S. by Bull HN Information Systems Inc., 
as successor in interest by change in name only to Honeywell Bull Inc. and Honeywell Information Systems Inc. .

                                          -----------------------------------------------------------

Permission to use, copy, modify, and distribute these programs and their documentation for any purpose and without
fee is hereby granted,provided that the below copyright notice and historical background appear in all copies
and that both the copyright notice and historical background and this permission notice appear in supporting
documentation, and that the names of MIT, HIS, BULL or BULL HN not be used in advertising or publicity pertaining
to distribution of the programs without specific prior written permission.
    Copyright 1972 by Massachusetts Institute of Technology and Honeywell Information Systems Inc.
    Copyright 2006 by BULL HN Information Systems Inc.
    Copyright 2006 by Bull SAS
    All Rights Reserved
